---
title: "PublicPrivate_Red"
author: "Jacob Levine"
date: "7/10/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Table of Contents:
0. Load Packages
1. Data Management
  1.1 Fire List
  1.2 Severity and Patch ID
  1.3 Ownership
  1.4 Topography
  1.5 Weather 
  1.6 Autocovariate calculation
  1.7 
  1.8 Cleaning
2. Data Exploration
3. Model Fitting
  3.1 For individual fires
  3.2 For each fire
4. Figure Generation

##0. Load Packages
```{r}

package.list <- c("raster", "rgdal", "reshape2", "ggplot2", "car", "MASS", "gstat", "spdep", "sf", "splitstackshape", "lme4", "speedglm", "paralell", "mixmeta")

lapply(package.list, require, character.only = TRUE)

```


##1. Data Management

```{r}
## set working directory
setwd("~/block_storage/Dropbox (Stephens Lab)")

## data management utility function
.ls.objects <- function (pos = 1, pattern, order.by,
                        decreasing=FALSE, head=FALSE, n=5) {
    napply <- function(names, fn) sapply(names, function(x)
                                         fn(get(x, pos = pos)))
    names <- ls(pos = pos, pattern = pattern)
    obj.class <- napply(names, function(x) as.character(class(x))[1])
    obj.mode <- napply(names, mode)
    obj.type <- ifelse(is.na(obj.class), obj.mode, obj.class)
    obj.prettysize <- napply(names, function(x) {
                           format(utils::object.size(x), units = "auto") })
    obj.size <- napply(names, object.size)
    obj.dim <- t(napply(names, function(x)
                        as.numeric(dim(x))[1:2]))
    vec <- is.na(obj.dim)[, 1] & (obj.type != "function")
    obj.dim[vec, 1] <- napply(names, length)[vec]
    out <- data.frame(obj.type, obj.size, obj.prettysize, obj.dim)
    names(out) <- c("Type", "Size", "PrettySize", "Length/Rows", "Columns")
    if (!missing(order.by))
        out <- out[order(out[[order.by]], decreasing=decreasing), ]
    if (head)
        out <- head(out, n)
    out
}

# shorthand
lsos <- function(..., n=10) {
    .ls.objects(..., order.by="Size", decreasing=TRUE, head=TRUE, n=n)
}

```


####1.2 Severity and Patch ID
```{r}

## set filepaths
fp_int <- "REMOTE/SevOwnership-main/Data/Intermediate/"
fp_orig <- "REMOTE/SevOwnership-main/Data/Original/"

## upload perimeter shapefile
pers <- st_read(paste0(fp_int, "study_pers.shp"))
fire.data <- as.data.frame(pers)[,1:10]

## get filenames from directory
files <- list.files(paste0(fp_orig, "so_cbi/"))

## reproject fire perimeter data
pers <- st_transform(pers, crs(raster(paste0(fp_orig, "so_cbi/", files[1]))))

## vegetation file
mixedcon <- raster("REMOTE/SevOwnership-main/Data/Intermediate/mixedcon.tif")

## reclass matrix for severity data
rc_matrix <- matrix(c(0, 2.25, 2.25, 3, 0, 1), nrow = 2)


####### OWNERSHIP PREP #########

## load in total ownership data 
ownership.shp <- st_read(paste0(fp_orig, "ownership15_1.shp"))
ownership.shp <- st_transform(ownership.shp, crs(raster(paste0(fp_orig, "so_cbi/", files[1]))))
ownership.shp <- st_make_valid(ownership.shp)
ownership.shp <- st_collection_extract(ownership.shp, type = "POLYGON")

##get summary stats
empty.raster <- raster(extent(pers), res = 0.0002694946)
ownership.fullraster <- fasterize(ownership.shp, empty.raster, field = "Own_Group")
ext <- extract(ownership.fullraster, pers)
ext.ul <- unlist(ext)
ext.ul <- ext.ul[!is.na(ext.ul)]
fun <- function(i) {
  prop <- sum(ext.ul == i) / length(ext.ul)
  return(prop)
}

## transform to dataframe in order to create grouping data
ownership.df <- as.data.frame(ownership.shp)
ownership.df$Group_ID <- NA

## create ownership type mapping: 0 = other, 1 = Private Industrial, 2 = Public Land
own_class_mapping <- data.frame(Own_Group = unique(ownership.df$Own_Group), Group_ID = c(rep(2, times = 10), 0, rep(2, times = 3)))

for (i in 1:14) {
  ownership.df[ownership.df$Own_Group == own_class_mapping$Own_Group[i], "Group_ID"] <- own_class_mapping$Group_ID[i]
}

ownership.shp$Group_ID <- ownership.df$Group_ID
ownership.shp <- st_collection_extract(ownership.shp, type = "POLYGON")

## do same for private industrial land
ownership_private.shp <- st_read(paste0(fp_orig, "Forest_Industry_Owners_18_2.shp"))
ownership_private.shp <- st_transform(ownership_private.shp, crs(raster(paste0(fp_orig, "so_cbi/", files[1]))))
ownership_private.shp <- st_make_valid(ownership_private.shp)
ownership_private.shp$Group_ID <- 1


##### TOPOGRAPHY PREP ####

## read in DEM  
DEM <- raster(paste0(fp_int, "output_srtm.tif"))
DEM <- projectRaster(DEM, res = res(raster(paste0(fp_orig, "so_cbi/", files[1]))), crs = crs(raster(paste0(fp_orig, "so_cbi/", files[1])))) ## reproject

## calculate TPI, this takes some time
TPI <- tpi(DEM, scale = 21) ## consider pixels within a 300m by 300m square

## calculate aspect
aspect <- terrain(DEM, "aspect", unit = "degrees", neighbors = 8)

## calculate slope
slope <- terrain(DEM, "slope", unit = "degrees", neighbors = 8)


```


####1.3 Process severity, ownership and topography data
```{R}

## function to process severity data
process_severity <- function(years, filelist, firelist) {
  
  ## initialize data frame
  severity.df <- data.frame(x = numeric(0), 
                            y = numeric(0), 
                            HS = numeric(0), 
                            fire_name <- character(0),
                            alarm_date <- integer(0),
                            cont_date <- integer(0),
                            year = numeric(0), 
                            objectid = numeric(0), 
                            other_distance = numeric(0), 
                            public_distance = numeric(0), 
                            private_distance = numeric(0), 
                            elevation = numeric(0), 
                            aspect = numeric(0), 
                            slope = numeric(0), 
                            tpi = numeric(0)) 
  
  class(severity.df$alarm_date) <- "Date"
  class(severity.df$cont_date) <- "Date"
  ## iterate over years
  for (i in 1:length(years)) {
    
    y <- years[i]
  
    f <- filelist[grepl(paste(fire.data[fire.data$year == y, "objectid"], collapse = "|"), filelist)]
    fire_names <- fire.data[fire.data$year == y, "fire_name"]
    
    ## loop over fires 
    for (fire in 1:length(f)) {
      
      severity <- raster(paste0(fp_orig, "so_cbi/", f[fire]))
      
      ## reproject mcf layer
      mcsub <- projectRaster(mixedcon, severity)
      
      ## process severity layer
      severity <- overlay(severity, mcsub, fun = function(x, y) {
        x[is.na(y[])] <- NA
        return(x)
      })
      
      severity <- reclassify(severity, rcl = rc_matrix)
      
      n.severity.df <- as.data.frame(severity, xy = TRUE, na.rm = TRUE)
      colnames(n.severity.df)[3] <- "HS"
      
      n.severity.df$fire_name <- fire.data[fire.data$year == y, "fire_name"][fire]
      n.severity.df$year <- y
      n.severity.df$objectid <- fire.data[fire.data$year == y, "objectid"][fire]
      
      
      ## process ownership distance data:
      ext <- extent(severity)
      empty.raster <- raster(ext, res = res(severity))
      ownership.raster <- fasterize(ownership.shp, empty.raster, field = "Group_ID")
      ownership_private.raster <- fasterize(ownership_private.shp, empty.raster, field = "Group_ID")
      ownership_merged.raster <- merge(ownership.raster, ownership_private.raster)
      ownership_merged.raster[is.na(ownership_merged.raster)] <- 0
      
      ## calculate distance to ownership types
      ownership_other.raster <- ownership_merged.raster
      ownership_other.raster[ownership_other.raster != 0] <- NA
      
      ## if no other ownership, assign large distance in order to eliminate from model
      if(any(!is.na(ownership_other.raster[]))) {
        
        other_distance.raster <- distance(ownership_other.raster)
        other_distance.raster <- overlay(other_distance.raster, severity, fun = function(x, y) {
          x[is.na(y[])] <- NA
          return(x)
        })
        other_distance.df <- as.data.frame(other_distance.raster, xy = FALSE, na.rm = TRUE)
        n.severity.df$other_distance <- other_distance.df$layer ## merge into dataframe
        
      }
      else {
        n.severity.df$other_distance <- 1e+15
      }
      
      
      ownership_public.raster <- ownership_merged.raster
      ownership_public.raster[ownership_public.raster != 2] <- NA
      

      ownership_public.raster[!is.na(ownership_public.raster)] <- 0
      public_distance.raster <- distance(ownership_public.raster)
      public_distance.raster <- overlay(public_distance.raster, severity, fun = function(x, y) {
        x[is.na(y[])] <- NA
        return(x)
      })
      public_distance.df <- as.data.frame(public_distance.raster, xy = FALSE, na.rm = TRUE)
      n.severity.df$public_distance <- public_distance.df$layer ## merge into dataframe

      
 
      ownership_private.raster[!is.na(ownership_private.raster)] <- 0
      private_distance.raster <- distance(ownership_private.raster)
      private_distance.raster <- overlay(private_distance.raster, severity, fun = function(x, y) {
        x[is.na(y[])] <- NA
        return(x)
      })
      private_distance.df <- as.data.frame(private_distance.raster, xy = FALSE, na.rm = TRUE)
      n.severity.df$private_distance <- private_distance.df$layer ## merge into dataframe
      
      
      ## TOPOGRAPHY processing
      n.elevation <- projectRaster(DEM, severity)
      n.elevation <- overlay(n.elevation, severity, fun = function(x, y) {
        x[is.na(y[])] <- NA
        return(x)
      })
      n.elevation.df <- as.data.frame(n.elevation, xy = FALSE, na.rm = TRUE)
      n.severity.df$elevation <- n.elevation.df$layer
      
      n.aspect <- projectRaster(aspect, severity)
      n.aspect <- overlay(n.aspect, severity, fun = function(x, y) {
        x[is.na(y[])] <- NA
        return(x)
      })
      n.aspect.df <- as.data.frame(n.aspect, xy = FALSE, na.rm = TRUE)
      n.severity.df$aspect <- n.aspect.df$layer
      
      n.slope <- projectRaster(slope, severity)
      n.slope <- overlay(n.slope, severity, fun = function(x, y) {
        x[is.na(y[])] <- NA
        return(x)
      })
      n.slope.df <- as.data.frame(n.slope, xy = FALSE, na.rm = TRUE)
      n.severity.df$slope <- n.slope.df$layer
      
      n.tpi <- projectRaster(TPI, severity)
      n.tpi <- overlay(n.tpi, severity, fun = function(x, y) {
        x[is.na(y[])] <- NA
        return(x)
      })
      n.tpi.df <- as.data.frame(n.tpi, xy = FALSE, na.rm = TRUE)
      n.severity.df$tpi <- n.tpi.df$layer
      ## bind new data
      severity.df <- rbind(severity.df, n.severity.df)
      
    }
    
  }  
  
  return(severity.df) ## return data frame
  
}

## function to split vector for parallelization
par_split <- function(x, cores) split(x, cut(seq_along(x), cores, labels = FALSE))

years_tosample <- par_split(unique(as.numeric(as.character(fire.data$year))), detectCores())

## run function (takes a long time and is memory intensive, needs to run on cluster)
severity_list <- mclapply(years_tosample, process_severity, mc.cores = detectCores(), filelist = files, firelist = fire.data)

full_data <- do.call(rbind, severity_list)

## cleanup
rm(severity_list)
gc()

```


####1.4 Weather
```{r}

## pre-process weather data
weather_grid <- raster(paste0(fp_int, "metdata_elevationdata.nc")) ## this is actually elevation data but at same grid as weather
weather_grid <- crop(weather_grid, DEM)
weather_grid.df <- as.data.frame(weather_grid, xy = TRUE, na.rm = TRUE)


## determine nearest weather grid point for each data point in full_data
neighbors <- nn2(weather_grid.df[,c("x", "y")], full_data[,c("x", "y")], k = 1)
full_data$weatherlat <- weather_grid.df[,"y"][neighbors$nn.idx]
full_data$weatherlong <- weather_grid.df[,"x"][neighbors$nn.idx]

## create unique weather x fire identifier
full_data$weather_ID <- paste0(full_data$objectid, "_", full_data$weatherlat, "_", full_data$weatherlong)

## create new dataset to loop over
weather_data <- data.frame(weather_ID = as.character(unique(full_data$weather_ID)))
weather_data$weather_ID <- as.character(weather_data$weather_ID)
weather_data$objectid <- as.numeric(unlist(strsplit(weather_data$weather_ID, "_")))[seq(1, nrow(weather_data)*3, by = 3)]
weather_data$lat <- as.numeric(unlist(strsplit(weather_data$weather_ID, "_")))[seq(2, nrow(weather_data)*3, by = 3)]
weather_data$long <- as.numeric(unlist(strsplit(weather_data$weather_ID, "_")))[seq(3, nrow(weather_data)*3, by = 3)]
weather_data <- weather_data[complete.cases(weather_data),]
weather_data$max_bi_7 <- NA
weather_data$mean_bi_7 <- NA


##set all end dates to +7
fire.data$alarm_plus_7 <- fire.data$alarm_date+7
fire.data$alarm_plus_10 <- fire.data$alarm_date+10

##errors in year data:
## ralston is from 2006 not 2007, confirmed by wikipedia
fire.data[fire.data$fire_name == "RALSTON", "year"] <- 2006
full_data[full_data$fire_name == "RALSTON", "year"] <- 2006

weather_data <- weather_data[!is.na(weather_data$objectid),]

## function to scrape weather data from web
pull_weather <- function(hs_fire, vs = c("bi"), range = "alarm_plus_7") {
  hs_fire <- as.data.frame(hs_fire)
  #Get dates and convert to proper format using regex
    sd=fire.data[fire.data$objectid == hs_fire$objectid,"alarm_date"]
    ##sd=gsub('^(.{4})(.*)$', '\\1-\\2', sd) #Insert dash (crazy regex)
    ##sd=gsub('^(.{7})(.*)$', '\\1-\\2', sd) #Insert dash
    ed=paste(fire.data[fire.data$objectid == hs_fire$objectid, range]) #End burn window 7 days after ignition
    ##ed=gsub('^(.{4})(.*)$', '\\1-\\2', ed) #Insert dash (crazy regex)
    ##ed=gsub('^(.{7})(.*)$', '\\1-\\2', ed) #Insert dash
    
    if (grepl("7", range)) r <- "7"
    else if (grepl("10", range)) r <- "10"
    else if (grepl("14", range)) r <- "14"
    
    full_vs <- c("tmmx", "tmmn", "rmax", "bi")
    fullname_vs <- 
      c("air_temperature","air_temperature","relative_humidity","burning_index_g")
    vs_full <- #API call needs full variable name as well
      fullname_vs[full_vs %in% vs]
    for(index in 1:length(vs)){ #Variable for loop
      v=vs[index]
      v_full=vs_full[index]
      
      #Key function: Create a string with the url for data download
      #This string includes the specific area (lat/long) and time window of interest. 
      #The area is specified by a bounding box,
      #with a 0.2 degree buffer around the fire centroid.
      #The resolution of the data is 0.04 degrees per pixel-width.
      v_link=paste0("https://www.reacchpna.org/thredds/ncss/MET/", v, "/", v, "_", 
                    fire.data[fire.data$objectid == hs_fire$objectid, "year"],
                    ".nc?var=",v_full,
                    "&north=",round(hs_fire$lat+0.2, 5),
                    "&west=",round(hs_fire$long-0.2, 5),
                    "&east=",round(hs_fire$long+0.2, 5),
                    "&south=",round(hs_fire$lat-0.2, 5),
                    "&disableProjSubset=on&horizStride=1",
                    "&time_start=",sd,"T00%3A00%3A00Z",
                    "&time_end=",ed,"T00%3A00%3A00Z&timeStride=1&accept=netcdf")
      
      dest <-  paste0("REMOTE/DATA/",v,".nc" )
      #Optional: Add lat/long centroid to data frame to check accuracy.
      #fire.list[f,"lat"]=gCentroid(hs_fire_ll)$y
      #fire.list[f,"long"]=gCentroid(hs_fire_ll)$x
      tmp <- #Download the relevant nc file from GridMet for the appropriate point and time
        try(download.file(url=v_link,destfile=dest, mode="wb"), silent=T)
      if(class(tmp)!="try-error"){ #CHECK download error: 
        #if the download produced an error (e.g. fires 251, 252), 
        #you will have to do it manually so skip the next bit.
        #otherwise, extract climate maxima/minima from the downloaded file.
        ncin <- nc_open(filename = dest) #Open the file you just downloaded
        lat <- ncvar_get(ncin,"lat",verbose=F) #Several pixels in sample
        lat_target <- #Find closest latitude pixel to fire centroid
          which.min(abs(lat - hs_fire$lat)) 
        lon <- ncvar_get(ncin,"lon",verbose=F)
        lon_target <- #Find closest latitude to fire centroid
          which.min(abs(lon - hs_fire$long)) 
        v_array <- ncvar_get(ncin,v_full)
        if(class(v_array)=="matrix"){ 
          #If there was only one burn day, so there's a matrix instead of an array
          #Duplicate the arrayso there's no error produced in maximum calculation
          v_array=replicate(2,v_array,simplify="array")
        }
        if(v=="tmmx"){
          #Get maximum high temperature during the burn window
        hs_fire[1,paste0("max_", v, "_", r)] <- #Convert from K to C
            max(v_array[lat_target,lon_target,])-273.15 
        }
        if(v=="tmmn"){
          #Get maximum low temperature during the burn window
          hs_fire[1,paste0("max_", v, "_", r)] <- #Convert from K to C
            max(v_array[lat_target,lon_target,])-273.15
        }
        #Get minimum high RH during the burn window
        if(v=="rmax"){
          hs_fire[1,paste0("min_", v, "_", r)] <-
            min(v_array[lat_target,lon_target,]) 
        }
        if(v=="bi"){
          #Get max burn index during burn window
          hs_fire[1,paste0("max_", v, "_", r)] <-
            max(v_array[lat_target,lon_target,]) 
          hs_fire[1,paste0("mean_", v, "_", r)] <-
            mean(v_array[lat_target,lon_target,])
        }
      } #END CHECK download error
    closeAllConnections() 
    unlink(dest) 
    }
  return(hs_fire)
}

n.cores <- detectCores()
for(f in seq(1, nrow(weather_data), by = n.cores)) { #Fire for loop
  
  indices <- seq(f, (f+n.cores-1))
  hs_fires <- split(weather_data[indices, ], seq(n.cores)) ## create list of rows
  
  weather.input <- tryCatch(mclapply(hs_fires, FUN = pull_weather, mc.cores = n.cores), warning =function(w) w, error = function(e) e)
  
  ## dont mess everything up if there is a warning
  if(!inherits(weather.input, "warning") & !inherits(weather.input, "error")) {
    
    weather_data[indices, ] <- as.data.frame(do.call(rbind, weather.input))
    write.csv(weather_data,"REMOTE/DATA/weather_data_NewAnalysis.csv")

    gc()
    print(paste0(indices[n.cores], " complete"))
    
  }
  
}

## sometimes the connection fails, retry these instances
misses <- as.numeric(rownames(weather_data[is.na(weather_data$max_bi_7), ]))

for(f in misses) { #Fire for loop
  
  indices <- misses[i:(i+n.cores-1)]
  indices <- indices[!is.na(indices)]
  hs_fires <- split(weather_data[indices, ], seq(length(indices))) ## create list of rows
  
  weather.input <- tryCatch(mclapply(hs_fires, FUN = pull_weather, mc.cores = n.cores), warning =function(w) w, error = function(e) e)
  
  ## dont mess everything up if there is a warning
  if(!inherits(weather.input, "warning") & !inherits(weather.input, "error")) {
    
    weather_data[indices, ] <- as.data.frame(do.call(rbind, weather.input))
    write.csv(centroids,"REMOTE/DATA/weather_data_NewAnalysis.csv")

    gc()
    print(paste0(indices[n.cores], " complete"))
    
  }
  
  i <- i+n.cores
}

## check if fully downloaded
length(as.numeric(rownames(centroids[is.na(weather_data$max_bi), ]))) == 0 ##g2g

## join weather data to full_data
full_data <- merge(full_data, weather_data[, c("weather_ID", "max_bi_10", "mean_bi_10","max_bi_7", "mean_bi_7")], by = "weather_ID", all = TRUE)
full_data <- full_data[complete.cases(full_data),]

```


####1.5 Calculate topographic indices and add ecoregions
```{r}
## remove unneeded columns
full_data <- full_data[, !(names(full_data) %in% c("weather_ID", "weatherlat", "weatherlong", "X"))]

## calculate heat load:
full_data$folded_aspect <- 180 - abs(full_data$aspect - 180)
full_data$folded_aspect <- (full_data$folded_aspect*pi)/180 ## convert to radians
full_data$rad.lat <- (full_data$y*pi)/180 ## convert to radians
full_data$rad.slope <- (full_data$slope*pi)/180 ## convert to radians
full_data$heat_load <- -1.467 + (1.582*cos(full_data$rad.lat)*cos(full_data$rad.slope)) + (-1.5*cos(full_data$folded_aspect)*sin(full_data$rad.slope)*sin(full_data$rad.lat)) + (-.262*sin(full_data$rad.lat)*sin(full_data$rad.slope)) + (.607*sin(full_data$folded_aspect)*sin(full_data$rad.slope))

full_data$heat_load <- exp(full_data$heat_load)

## remove unneeded columns
full_data <- full_data[, !(names(full_data) %in% c("rad.lat", "rad.slope", "folded_aspect"))]



## load ecoregions data
ecoregions <- st_read("REMOTE/DATA/ca_eco_l3.shp")
ecoregions <- st_transform(ecoregions, crs(raster(paste0(fp_orig, "so_cbi/", files[1]))))

## add ecoregions data to full_data
ext <- extent(weather_grid)
empty.raster <- raster(ext, res = res(weather_grid))
ecoregions.raster <- fasterize(ecoregions, empty.raster, field = "US_L3CODE")
ecoregions.df <- as.data.frame(ecoregions.raster, xy = TRUE, na.rm = TRUE)

neighbors <- nn2(ecoregions.df[,c("x", "y")], full_data[,c("x", "y")], k = 1)
full_data$ecoregion <- ecoregions.df[,"layer"][neighbors$nn.idx]

full_data$ecoregion <- factor(full_data$ecoregion)

```


####1.5 Cleaning
```{r}

## reclass factors as factors
full_data$year <- as.factor(full_data$year)

## create ownership column for cleaning purposes
full_data$ownership <- NA
full_data[full_data$other_distance == 0, "ownership"] <- "other"
full_data[full_data$public_distance == 0, "ownership"] <- "public"
full_data[full_data$private_distance == 0, "ownership"] <- "private"

## subset data to ensure comparability between ownerships
full_data <- full_data[full_data$elevation < max(full_data[full_data$ownership == "private", "elevation"]), ]
full_data <- full_data[full_data$elevation > min(full_data[full_data$ownership == "private", "elevation"]), ]
full_data <- full_data[full_data$slope < max(full_data[full_data$ownership == "private", "slope"]), ]
full_data <- full_data[full_data$tpi < max(full_data[full_data$ownership == "private", "tpi"]), ]
full_data <- full_data[full_data$tpi > min(full_data[full_data$ownership == "private", "tpi"]), ]


## normalize data values
full_data$slope.norm <- (full_data$slope - mean(full_data$slope))/sd(full_data$slope)
full_data$elevation.norm <- (full_data$elevation - mean(full_data$elevation))/sd(full_data$elevation)
full_data$heat_load.norm <- (full_data$heat_load - mean(full_data$heat_load))/sd(full_data$heat_load)
full_data$tpi.norm <- (full_data$tpi - mean(full_data$tpi))/sd(full_data$tpi)
full_data$max_bi.norm <- (full_data$max_bi - mean(full_data$max_bi))/sd(full_data$max_bi)
full_data$mean_bi.norm <- (full_data$mean_bi - mean(full_data$mean_bi))/sd(full_data$mean_bi)
full_data$max_bi_7.norm <- (full_data$max_bi_7 - mean(full_data$max_bi_7))/sd(full_data$max_bi_7)
full_data$mean_bi_7.norm <- (full_data$mean_bi_7 - mean(full_data$mean_bi_7))/sd(full_data$mean_bi_7)
write.csv(full_data, "REMOTE/DATA/full_data.csv")
```




******************************* MODEL FITTING ********************************


##2 Model Fitting

####2.1 Preparation and naive model run
```{r}

full_data <- fread("REMOTE/DATA/full_data.csv")

## take a stratified random sample of 25% from each fire to make computation feasible
bootstrap_data <- as.data.frame(stratified(indt = full_data, group = "objectid", size = (0.25)))

## function to calculate values of 1 for a range of interest and tolerance
calc_q <- function(range, val = 0.001) -log(val)/range

## calculate qs of interest
ranges <- as.matrix(seq(0, 8, 0.5))
qs <- apply(X = ranges, MARGIN = 1, FUN = calc_q)

## function to attenuate distances for full column
att <- function(dist, q) exp(-q*dist)
att_distance <- function(dists, q) apply(X = as.matrix(dists), MARGIN = 1, FUN = att, q = q)

## function to caluclate weighted average proximity
avg_dist <- function(dist.prime, dist1, dist2) dist.prime * (dist.prime/(sum(c(dist.prime, dist1, dist2))))

## function to fit model and extract likelihood for a value of q
check_q <- function(data, q) {
  
  data$other_dist_att <- exp(-q*(data$other_distance/1000))
  data$public_dist_att <- exp(-q*(data$public_distance)/1000)
  data$private_dist_att <- exp(-q*(data$private_distance)/1000)
  
  data[data$other_dist_att == 1, "other_dist_att"] <- 0
  data[data$public_dist_att == 1, "public_dist_att"] <- 0
  data[data$private_dist_att == 1, "private_dist_att"] <- 0
  
  avg_other <- mapply(FUN = avg_dist, dist.prime = data[,"other_dist_att"], dist1 = data[,"public_dist_att"], dist2 = data[,"private_dist_att"], SIMPLIFY = TRUE)
  avg_public <- mapply(FUN = avg_dist, dist.prime = data[,"public_dist_att"], dist1 = data[,"other_dist_att"], dist2 = data[,"private_dist_att"], SIMPLIFY = TRUE)
  avg_private <- mapply(FUN = avg_dist, dist.prime = data[,"private_dist_att"], dist1 = data[,"other_dist_att"], dist2 = data[,"public_dist_att"], SIMPLIFY = TRUE)
  
  data$avg_other <- avg_other
  data$avg_public <- avg_public
  data$avg_private <- avg_private
  
  print("running model")

  model <- speedglm(HS ~ ownership + avg_other + avg_public + avg_private + slope.norm + elevation.norm + heat_load.norm + tpi.norm + ecoregion + objectid, data = data, family = binomial(), model = FALSE, y = FALSE)
  
  print("finished model run")
  
  logLik <- logLik(model)
  
  return(list(q, logLik))
  
}


test.models <- vector("list", length(qs))
for (i in 1:length(qs)) {
  
  print(paste("starting model:"), i)
  
  test.models[[i]] <- check_q(q = qs[i], data = bootstrap_data)
  
  print(paste("finished model:"), i)
  
  
}


df.distance_test <- data.frame(q = sapply(test.models, "[[", 1), logLik = sapply(test.models, "[[", 2))

## model with best fitting q value based on max dist of 6km
q <- df.distance_test[df.distance_test$logLik == max(df.distance_test$logLik), "q"]

## do calculations and add to data.frame
bootstrap_data$other_dist_att <- exp(-q*(bootstrap_data$other_distance/1000))
bootstrap_data$public_dist_att <- exp(-q*(bootstrap_data$public_distance)/1000)
bootstrap_data$private_dist_att <- exp(-q*(bootstrap_data$private_distance)/1000)
  
bootstrap_data[bootstrap_data$other_dist_att == 1, "other_dist_att"] <- 0
bootstrap_data[bootstrap_data$public_dist_att == 1, "public_dist_att"] <- 0
bootstrap_data[bootstrap_data$private_dist_att == 1, "private_dist_att"] <- 0
  
avg_other <- mapply(FUN = avg_dist, dist.prime = bootstrap_data[,"other_dist_att"], dist1 = bootstrap_data[,"public_dist_att"], dist2 = bootstrap_data[,"private_dist_att"], SIMPLIFY = TRUE)
avg_public <- mapply(FUN = avg_dist, dist.prime = bootstrap_data[,"public_dist_att"], dist1 = bootstrap_data[,"other_dist_att"], dist2 = bootstrap_data[,"private_dist_att"], SIMPLIFY = TRUE)
avg_private <- mapply(FUN = avg_dist, dist.prime = bootstrap_data[,"private_dist_att"], dist1 = bootstrap_data[,"other_dist_att"], dist2 = bootstrap_data[,"public_dist_att"], SIMPLIFY = TRUE)
  
bootstrap_data$avg_other <- avg_other
bootstrap_data$avg_public <- avg_public
bootstrap_data$avg_private <- avg_private
  
bootstrap_data$objectid <- as.factor(bootstrap_data$objectid)
bootstrap_data$ecoregion <- as.factor(bootstrap_data$ecoregion)

fwrite(bootstrap_data, "REMOTE/BOOTSTRAP/bootstrap_data.csv")

## run naive_glm on full data to get base parameter estimates
naive_glm <- glm(HS ~ ownership + avg_other + avg_public + avg_private + slope.norm + elevation.norm + heat_load.norm + tpi.norm + ecoregion + objectid, data = bootstrap_data, family = binomial)


## now we estimate the scale of residual autocorrelation by vitting a semivariogram
## transform to UTM zone 10 in meters bc easier to work with
sp_bs <- st_as_sf(bootstrap_data, coords = c("x", "y"), crs = crs(DEM))
sp_bs <- st_transform(sp_bs, crs = 32610)

bootstrap_data <- data.frame(bootstrap_data)
bootstrap_data[,c("x", "y")] <- st_coordinates(sp_bs)
## create variogram to estimate maximum range of autocorrelation
resid.rm.glm <- residuals(naive_glm)
df.resid <- data.frame(z = resid.rm.glm, x = bootstrap_data$x, y = bootstrap_data$y)
v1 <- variogram(z~1, data = df.resid, locations = ~x+y, cutoff = 3000)
f1 <- fit.variogram(v1, vgm("Sph"))
max.dist <- f1$range[2]
bb <- ceiling(max.dist) ##round up to nearest whole number


bootstrap_data <- bootstrap_data[sample(nrow(bootstrap_data)),]
```


####2.2 Spatial block bootstrapping
``` {r}
## Define study area and determine number of bounding boxes. There is a bit of weirdness because of overlapping fire boundaries.
## create vector of objectids
bootstrap_data <- fread("REMOTE/BOOTSTRAP/bootstrap_data.csv")
bootstrap_data <- data.table(bootstrap_data)

## get list of objectids to stratify over
objectids <- unique(bootstrap_data$objectid)

## create empty dataframe for potential sample points
sample_points <- data.frame(objectid = objectids, 
                            xmin = rep(0, times = length(objectids)), 
                            xmax = rep(0, times = length(objectids)), 
                            ymin = rep(0, times = length(objectids)), 
                            ymax = rep(0, times = length(objectids)),
                            num_boxes = rep(0, times = length(objectids)))

## loop over individual fires:
for (i in 1:length(objectids)) {
  
  ## determine minimum rectangle
  fire_area <- list(xmin = min(bootstrap_data[objectid == as.numeric(as.character(objectids[i])), "x"]),
                    xmax = max(bootstrap_data[objectid == as.numeric(as.character(objectids[i])), "x"]),
                    ymin = min(bootstrap_data[objectid == as.numeric(as.character(objectids[i])), "y"]),
                    ymax = max(bootstrap_data[objectid == as.numeric(as.character(objectids[i])), "y"]))
   
  ## divide study area into boxes of size bb
  box_centers <- expand.grid(x = seq(fire_area$xmin, fire_area$xmax, by = bb) + bb/2,
                             y = seq(fire_area$ymin, fire_area$ymax, by = bb) + bb/2)
  
  
  num_boxes <- nrow(box_centers)
  
  sample_points[i, 2] <- min(box_centers$x)
  sample_points[i, 3] <- max(box_centers$x)
  sample_points[i, 4] <- min(box_centers$y)
  sample_points[i, 5] <- max(box_centers$y)
  sample_points[i, 6] <- num_boxes

}

## req fields for analysis:
fields <- c("HS", 
            "ownership",
            "avg_other",
            "avg_public", 
            "avg_private",
            "slope.norm", 
            "elevation.norm", 
            "heat_load.norm", 
            "tpi.norm", 
            "ecoregion", 
            "year", 
            "objectid")


## function to pull data given spatial extent, fire name, and fields
pull_data <- function(xcoord, ycoord, data, bb, objid) {
  
  p.data <- data[objectid == as.numeric(as.character(objid))]
  
  p.data <- p.data[x > (xcoord-(bb/2)) & x < (xcoord+(bb/2)) & y > (ycoord-(bb/2)) & y < (ycoord+(bb/2))]
  
  return(p.data)
  
}


## function to randomly choose boxes and pull data:
rpull <- function(nbox, objectid, data, bb, sample_points) {
  
  xcoords <- runif(nbox, sample_points[sample_points$objectid == objectid, "xmin"], sample_points[sample_points$objectid == objectid, "xmax"])
  ycoords <- runif(nbox, sample_points[sample_points$objectid == objectid, "ymin"], sample_points[sample_points$objectid == objectid, "ymax"])
  
  bs.data <- mapply(xcoords, 
                ycoords, 
                FUN = pull_data, 
                MoreArgs = list(data = data, bb = bb, objid = objectid),
                SIMPLIFY = FALSE)
  
  out <- do.call(rbind, bs.data)
  
  return(out)
  
}

## set number of bootstraps 
n <- 300

## create empty dataframe of fitted coefficients:
cols <- rownames(coef(summary(naive_glm)))
bs.coefs <- data.frame(matrix(NA, nrow = 1, ncol = length(cols)))
colnames(bs.coefs) <- cols
bs.coefs <- bs.coefs[0,] ## janky janky I'm a bad programmer

## write function to create bootstrap dataset, run model, append estimates to dataframe
run_bootstrap <- function(iter) {
  
  bs.data <- as.data.frame(do.call(rbind, mcmapply(objectid = sample_points$objectid, 
                                               nbox = sample_points$num_boxes, 
                                               FUN = rpull, 
                                               SIMPLIFY = FALSE,
                                               MoreArgs = list(data = bootstrap_data,
                                                               bb = bb,
                                                               sample_points = sample_points),
                                               mc.cores = detectCores())))
  
  print("starting glm")
  
  bs.glmm <- speedglm(HS ~ ownership + avg_other + avg_public + avg_private + slope.norm + elevation.norm + heat_load.norm + tpi.norm + ecoregion + objectid, 
                      data = bs.data, 
                      family = binomial(),
                      y = FALSE,
                      model = FALSE,
                      fitted = FALSE)
  
  coefs <- as.data.frame(matrix(coef(summary(bs.glmm))[, "Estimate"],
                                nrow = 1))
  rm(bs.glmm)
  gc()
  
  colnames(coefs) <- cols
  
  return(coefs)
  
}

## run bootstraps, can be done in parallel but my poor computer could not handle it
## a note - when doing this with speedglm I had memory clearing issues. gc() did not remove all the memory, and so in order to fully clear the memory between runs I had to restart R. I did so by looping on an alternative script which included calls to restart R after each iteration. (See below)
for(i in 1:(round(n/1))) {
  
  if (i == 1) print("starting bootstrap")
  
  coef.list <- mclapply(1, run_bootstrap, mc.cores = 1)
  n.coefs <- as.data.frame(do.call(rbind, coef.list))
  bs.coefs <- rbind(bs.coefs, n.coefs)
  
  write.csv(bs.coefs, file = "REMOTE/BOOTSTRAP/bootstrap_coefs_newerdata.csv")
  
  gc()
  
  print(paste0("completed iteration: ", i*7))
  
  print(Sys.time())
}


## this is the janky way to force R to restart after each iteration
## first save workspace image
save.image(file = "bsd.Rdata")
## then run external file (contents below)
source("run_bootstrap.R")

```

Here are the contents of the external file "run_bootstrap.R"

```{R}

library(rstudioapi)
library(parallel)
library(data.table)
library(speedglm)

load("bsd.Rdata")
bs.coefs <- read.csv("REMOTE/BOOTSTRAP/bootstrap_coefs_newerdata.csv")

print("starting bootstrap")
set.seed(NULL)
coef.list <- run_bootstrap(1)
print(coef.list)
colnames(bs.coefs) <- colnames(coef.list)
bs.coefs <- rbind(bs.coefs, coef.list)
write.csv(bs.coefs, file = "REMOTE/BOOTSTRAP/bootstrap_coefs_newerdata.csv", row.names = FALSE)

gc()

print(paste0("completed iteration: ", nrow(bs.coefs)))

print(Sys.time())

if (nrow(bs.coefs) < 300) {
  
  restartSession(command='source("run_bootstrap.R")')
  
}


```


####2.3 post-hoc weather analysis
```{r}
## weather data analysis

## get random effects estimates for objectids
## colnames:
rfnames <- c("objectid113", names(bs.coefs[16:168]))

## get corresponding objectids and years
## rfobj <- sapply(strsplit(rfnames, ":"), "[[", 2)
## rfyear <- sapply(strsplit(rfnames, ":"), "[[", 1)

## get average rf estimates
rfobj.est<- c(0, colMeans(bs.coefs[,16:168]))

## create data.frame
rf.df <- data.frame(objectid = rfnames, estimate = rfobj.est)


## get average burn index
max_bi <- aggregate(max_bi ~ objectid, data = bootstrap_data, FUN = mean)

rf.df$max_bi <- max_bi$max_bi

rf.df$S <- sapply(bs.coefs[,c(1, 16:168)], sd)

bimod <- mixmeta(estimate ~ max_bi, random = ~1|objectid, S = rf.df$S, data = rf.df)
bimod.sum <- summary(bimod)
## nada

## generate plot:
ggplot(data = rf.df, aes(x = max_bi, y = estimate)) +
  geom_point() +
  geom_abline(intercept = coef(bimod.sum)["(Intercept)", "Estimate"],
              slope = coef(bimod.sum)["max_bi", "Estimate"],
              linetype = "dashed",
              alpha = 0.5) + 
  ylab("Fire Effect (log odds)") +
  xlab("Maximum Burn Index") +
  theme(axis.line = element_line(size = 0.75, color = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        plot.title = element_text(size = 16))

```






